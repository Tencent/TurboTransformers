We show Transformer Decoder inference performance here.

For a translation demo [TurboNLP/Translate-Demo](https://github.com/TurboNLP/Translate-Demo "translate"),
Turbo will bring 15.9% performance improvements on RTX 2060 GPU.

We are still working on decoder model optimization.
